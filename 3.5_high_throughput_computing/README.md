# High Throughput Computing

High-throughput computing (HTC) workloads are characterized by large numbers of small jobs. These frequently involve parameter sweeps where the same type of calculation is done repeatedly with different input values or data processing pipelines where an identical set of operations is applied to many files. This session covers the characteristics and potential pitfalls of HTC, job bundling, the Open Science Grid and the resources available through the Partnership to Advance Throughput Computing (PATh).

- [Parallel paradigms: HPC vs. HTC](PARALLEL.md)
- [Batch job arrays and dependencies](ARRAYS.md)
- [Ad-hoc job/task bundling](BUNDLING.md)
- [Distributed high-throughput computing](DHTC.md)

#

[Marty Kandes](https://github.com/mkandes), Computational & Data Science Research Specialist, HPC User Services Group, SDSC
